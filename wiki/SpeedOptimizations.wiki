#summary Source code changes in FastRandomForest compared to original RF implementation in Weka.
#labels Phase-Design,Featured

= VERSION 0.7 (first public release) =

The following changes were made over Weka's RandomForest implementation (in Weka
developer branch nightly snapshot of 31-Oct-2007).

The FastRfBagging class (replaces weka.classifiers.meta.Bagging):

  * The new method resampleFastRf has functionality similar to resampleWithWeights, with the following difference: When an instance is to be sampled multiple times, resampleWithWeights would add multiple copies of the instance to the newData; resampleFastRf increases the weight (by 1) for the copy already present. This reduces the number of instances by ~30%, speeding things up. The trees' splitting criteria should work equally on this kind of dataset because they explicitely take weights into account, however the forests produced are a bit different when this sampling procedure is used.  I'm not sure what exactly causes the difference, however the crossvalidation performance of the forest did not seem to deteriorate on my tests.
  * The FastRfBagging now sorts all instances on every attribute and stores this data into each bag's sortedIndices field, adjusting for instances absent from  the bag; this means the sort is now performed once per forest, and not once per RandomTree (as in ordinary RandomForest).

The FastRfInstances class (inherits from weka.core.Instances):
  * The resampleFastRf now generates 'bags' of type FastRfInstances, which differs from ordinary Instances only in having 2 additional fields: int[][] sortedIndices and int[] whatGoesWhere.

The FastRandomForest class (replaces weka.classifiers.trees.RandomForest):
  * This class differs from ordinary RandomForest basically only in using FastRfBagger with FastRfTree, instead of Bagger with RandomTree. 

The FastRfTree class (replaces weka.classifiers.trees.RandomTree):
  * The FastRfTree has a different way of handling instances with missing values in attribute chosen for the split. The RandomTree used to 'split up' such an instance in two instances, with weights that are proportional to size of each branch at splitting point. Instead, FastRfTree assignes the instance to one of the branches at random, with bigger branches having a higher probability of getting the instance. This allows us to skip creating new arrays of instance weights and passing them down the tree, as the weights do not change anymore. My reasoning was that such an approximation was acceptable when a forest with lots (>100) of trees was created, although it may not be on a single tree.
  * The splitData procedure splits int[][] sortedIndices into two (or more for categorical split attributes) int[][][] subsetIndices. It was quite slow before because it checked whether an instance's split attribute value was above or below splitting threshold for each attribute in sortedIndices. Now, this checking is done only once, and results are stored into the dataset's whatGoesWhere array (dataset must be of type FastRfInstances). 
  * As a consequence of the above, the exact branch sizes (even with instances having unknowns in the split attribute) are known in advance so subsetIndices arrays don't have to be 'resized' (i.e. a new shorter copy of each one created and the old one GCed â€“ quite a lot of data being shuffled around).




= VERSION 0.71 =

The FastRfTree class:
During the normal training of the forest, a lot of instances of FastRfTree
are being created - one per split per tree. Since the trees in a Random 
Forest normally contain n-1 splits each (for n instances), it could pay off
to make 'lean' versions of the FastRfTree that has less fields, and therefore
a smaller memory footprint.
  * m_MotherForest (FastRandomForest) was introduced
  * m_KValue (int) was removed; a call to getKValue looks the value up by asking the m_MotherForest its KValue; setKValue was removed
  * ditto for m_MaxDepth and getMaxDepth
  * ditto for m_Info (no getter/setter)
  * m_MinNum is now a static final field always = 1; it was never set to  another value before anyway
  * m_RandomSeed and its getter and setter have been removed; instead, the FastRfBagging passes a random seed it has generated as a parameter of the BuildClassifier function - later, a Random object is generated from the seed and the seed is not used anymore. As a consequence of this, FastRfTree can't implement Randomizable anymore.
  * Some methods which are likely to go unused have been omitted: toGraph(), toString(), and leafString() - this doesn't help with size of trees in memory, I've done it just to clean up clutter. 
  * The check whether the default classifier (ZeroR) should be built was moved to FastRandomForest's buildClassifier(); and check if it should be used in testing to FastRandomForest's distributionForInstance(). So there is no more need for m_ZeroR field in FastRfTrees.

The Benchmark class was introduced, useful for quick comparisons (speed-wise
and accuracy-wise) to the weka.trees.RandomForest.


= VERSION 0.72 =

The FastRfTree class:
  * m_Distribution field, and all references to it are removed; it was used only localy in buildTree() as a reference to another array. Also, in leaves m_Distribution held (quite useless) copies of the m_ClassProbs.
  * in buildTree(), local array classProbs was unnecesarily deep copied to m_classProbs in every split; now it's only copied by reference, and only when a leaf is made. It's also normalized only in leaves (before, it was normalized in every node). To do:
    * last change sometimes causes an error, as the split function can produce an empty leaf (which it shouldn't!) and then there is no m_classProbs of the parent leaf to look up - it wasn't saved!! 


= VERSION 0.8 =

The FastRfInstances class was removed.

The DataCache class was introduced:
  * The DataCache stores a dataset that FastRandomTrees use for training. The data points are stored in a single-precision float array indexed by attribute first, and by instance second, allowing faster access by FastRandomTrees. 
  * The DataCache can seed a random number generator from the stored data.
  * The DataCache creates bootstrap samples of data. This functionality was removed from FastRfBagging class.
    * There is a subtle change in the way sampling is performed; before, if an instance had higher weight, it was more likely to get sampled multiple times, and its weight in the bagged dataset would increase in multiples of 1 (i.e. 1.0, 2.0, 3.0...) Now, the sampling frequency is independant of instance original weight, and its weight in the sampled data increases in multiples of the original weight, for example 1.5, 3.0, 4.5... This ensures the bagging procedure conserves class proportions in the bagged data.
  * The DataCache also computes and stores the sorted order of the instances by any attribute. This functionality was removed from FastRfBagging class. Due to the way that the DataCache is organized, the sortedInstances arrays don't have to be readjusted in making bootstrap samples.

The FastRfUtils class was introduced:
  * Contains functions for sorting single-precision float arrays used in DataCache, and for normalization of double arrays.

The FastRfTree class was renamed to FastRandomTree.


= VERSION 0.9 =

The SplitCriteria class was introduced.
  * EntropyConditionedOnRows() and entropyOverColumns() functions are made a bit faster by avoiding normalization for total number of instances.
  * The fastLog2() function was introduced as a rough approximation of log base 2 logarithms by bitwise operations on the internal representation of floating point numbers. It is approx. 4 times faster than Math.log(), relieving a performance bottleneck of FastRandomTree.

Changes in the FastRandomTree class:
  * The value of splitting criterion before split (entropyOverColumns) is not re-calculated for each attribute (because it doesn't change between attributes) in the distribution(), but only once in buildTree()
  * Also, when searching for best split points, the entropyOverColumns is not subtracted from entropyConditionedOnRows() in every possible split point, only in the best one in the buildTree().
  * In the distribution() function:
    * before it was possible to create a split 'in the middle' of the first instance 0, which would result in empty nodes after the split and cause  errors after the (otherwise unnecessary) m_classProbs was removed in 0.71. This 'empty node problem' is now fixed.
    * dist[][] is now computed only after the split point has been found, and not updated continually by copying from currDist.
    * instance 0 is now skipped when looking for split points, as the split point 'before instance 0' is not sensible and entropy does not need to be computed.
